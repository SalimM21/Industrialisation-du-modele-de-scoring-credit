# Industrialisation-du-modele-de-scoring-credit => Scoring MLOps Platform

## ğŸ¯ Objectif
Cette plateforme permet de **prÃ©parer, entraÃ®ner, dÃ©ployer et monitorer des modÃ¨les de scoring crÃ©dit**. Elle inclut lâ€™ingestion de donnÃ©es en streaming, lâ€™entraÃ®nement multi-modÃ¨les, le suivi MLflow, lâ€™industrialisation via API FastAPI/Docker et la surveillance de dÃ©rive via Evidently AI.

---

## ğŸ—‚ï¸ Structure du projet

```bash
credit_scoring_mlops/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # DonnÃ©es brutes (ingÃ©rÃ©es depuis Kafka / S3)
â”‚   â”œâ”€â”€ processed/               # DonnÃ©es nettoyÃ©es et prÃªtes pour entraÃ®nement
â”‚   â”œâ”€â”€ reference/               # Jeu de rÃ©fÃ©rence pour dÃ©tection de dÃ©rive
â”‚   â””â”€â”€ new_data/                # DonnÃ©es rÃ©centes pour recalibrage pÃ©riodique
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploration.ipynb        # Analyse exploratoire initiale
â”‚   â”œâ”€â”€ model_comparison.ipynb   # Tests de diffÃ©rents modÃ¨les (LR, XGBoost, NN)
â”‚   â””â”€â”€ monitoring_drift.ipynb   # DÃ©tection et visualisation des dÃ©rives Evidently
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_preprocessing/
â”‚   â”‚   â”œâ”€â”€ preprocessing.py     # Nettoyage, encodage, imputation, scaling
â”‚   â”‚   â”œâ”€â”€ feature_selection.py # SÃ©lection des variables pertinentes
â”‚   â”‚   â””â”€â”€ split_data.py        # Split train/validation/test
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_lr.py          # EntraÃ®nement du modÃ¨le Logistic Regression
â”‚   â”‚   â”œâ”€â”€ train_xgb.py         # EntraÃ®nement XGBoost
â”‚   â”‚   â”œâ”€â”€ train_nn.py          # EntraÃ®nement du modÃ¨le neuronal
â”‚   â”‚   â””â”€â”€ evaluate_models.py   # Comparaison AUC/F1 et sÃ©lection du meilleur modÃ¨le
â”‚   â”‚
â”‚   â”œâ”€â”€ export/
â”‚   â”‚   â”œâ”€â”€ export_model.py      # Conversion modÃ¨le en ONNX / PMML
â”‚   â”‚   â””â”€â”€ register_mlflow.py   # Enregistrement modÃ¨le final dans MLflow
â”‚   â”‚
â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py          # API FastAPI : endpoint `/predict`
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py       # SchÃ©mas Pydantic pour les entrÃ©es utilisateur
â”‚   â”‚   â”‚   â””â”€â”€ utils.py         # Fonctions auxiliaires (chargement modÃ¨le, logs)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Dockerfile           # Dockerfile pour containeriser lâ€™API
â”‚   â”‚   â”œâ”€â”€ requirements.txt     # DÃ©pendances API (FastAPI, ONNXRuntimeâ€¦)
â”‚   â”‚   â”œâ”€â”€ entrypoint.sh        # Script de dÃ©marrage Docker (optionnel)
â”‚   â”‚   â””â”€â”€ test_api.py          # Tests unitaires de lâ€™API avec pytest
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ drift_detection.py   # VÃ©rification automatique des dÃ©rives (Evidently)
â”‚   â”‚   â”œâ”€â”€ retrain_trigger.py   # Script de recalibrage pÃ©riodique
â”‚   â”‚   â””â”€â”€ generate_report.py   # GÃ©nÃ©ration du dashboard HTML Evidently
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ spark_session.py     # Initialisation Spark pour preprocessing
â”‚       â”œâ”€â”€ config.yaml          # ParamÃ¨tres globaux (chemins, hyperparams, seuils)
â”‚       â””â”€â”€ logger.py            # Configuration centralisÃ©e des logs
â”‚
â”œâ”€â”€ ci_cd/
â”‚   â”œâ”€â”€ github_actions.yml       # Workflow GitHub Actions (build, test, deploy)
â”‚   â”œâ”€â”€ docker-compose.yml       # Lancement multi-services (API + MLflow + monitoring)
â”‚   â””â”€â”€ Jenkinsfile              # (Optionnel) Pipeline Jenkins Ã©quivalent
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ xgb_credit_model.pkl     # ModÃ¨le XGBoost sauvegardÃ© (Pickle)
â”‚   â”œâ”€â”€ xgb_credit_model.onnx    # ModÃ¨le exportÃ© en ONNX
â”‚   â”œâ”€â”€ model_metadata.json      # MÃ©tadonnÃ©es du modÃ¨le (version, date, mÃ©triques)
â”‚   â””â”€â”€ mlruns/                  # Dossier de tracking MLflow (expÃ©riences)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_preprocessing.py    # Tests unitaires sur le preprocessing
â”‚   â”œâ”€â”€ test_training.py         # Tests unitaires sur l'entraÃ®nement
â”‚   â”œâ”€â”€ test_export.py           # VÃ©rifie lâ€™intÃ©gritÃ© du modÃ¨le exportÃ©
â”‚   â”œâ”€â”€ test_api.py              # Tests sur les endpoints FastAPI
â”‚   â””â”€â”€ test_monitoring.py       # Tests de dÃ©tection de dÃ©rive
â”‚
â”œâ”€â”€ dashboards/
â”‚   â”œâ”€â”€ credit_model_monitoring.html  # Dashboard Evidently gÃ©nÃ©rÃ©
â”‚   â””â”€â”€ drift_report.html             # Rapport de dÃ©rive complet
â”‚
â”œâ”€â”€ docker-compose.yml           # Orchestration (API + MLflow + PostgreSQL)
â”œâ”€â”€ README.md                    # Documentation du projet complet
â”œâ”€â”€ requirements.txt             # DÃ©pendances Python globales
â””â”€â”€ Makefile                     # Commandes automatiques (build, test, deploy)
```
---


**Description rapide :**

- **data/** : jeux de donnÃ©es bruts et transformÃ©s.  
- **notebooks/** : analyses exploratoires et tests de modÃ¨les.  
- **src/** : code source (prÃ©processing, entraÃ®nement, export, dÃ©ploiement, monitoring).  
- **ci_cd/** : pipelines CI/CD pour build et dÃ©ploiement.  
- **models/** : modÃ¨les sauvegardÃ©s (Pickle, ONNX) + historiques MLflow.  
- **tests/** : tests unitaires pour chaque Ã©tape du pipeline.  
- **dashboards/** : dashboards de suivi de performance et dÃ©rive.  

---

## âš™ï¸ FonctionnalitÃ©s principales

1. **PrÃ©paration des donnÃ©es** : nettoyage, imputation, encodage, scaling via PySpark / Pandas.  
2. **EntraÃ®nement multi-modÃ¨les** : Logistic Regression, XGBoost, Neural Network.  
3. **Tracking et versioning** : MLflow pour enregistrer paramÃ¨tres, mÃ©triques et modÃ¨les.  
4. **Export industriel** : modÃ¨les exportÃ©s en ONNX/PMML pour dÃ©ploiement.  
5. **API REST FastAPI** : prÃ©dictions en temps rÃ©el.  
6. **Containerisation Docker** : dÃ©ploiement reproductible.  
7. **CI/CD** : build, test et dÃ©ploiement automatisÃ©s (GitHub Actions / Jenkins).  
8. **Monitoring & recalibrage** : Evidently AI pour dÃ©rive de donnÃ©es et dÃ©clenchement automatique de rÃ©entraÃ®nement.

---

## ğŸš€ Pipeline global

```mermaid
graph TD
A[Kafka / S3 Data] --> B[PySpark Preprocessing]
B --> C[Model Training (LR / XGBoost / NN)]
C --> D[Tracking MLflow]
D --> E[Export ONNX / PMML]
E --> F[FastAPI + Docker Deployment]
F --> G[CI/CD Pipeline (GitHub Actions)]
F --> H[Monitoring & Drift Detection (Evidently AI)]
H --> I[Retrain Trigger (Automatic Recalibration)]
```
---

## ğŸ› ï¸ Installation

```bash
# Cloner le projet
git clone https://github.com/votre_repo/credit_scoring_mlops.git
cd credit_scoring_mlops

# Installer les dÃ©pendances Python
pip install -r requirements.txt

# Construire l'image Docker
docker build -t credit_scoring_api:latest ./src/deployment/

# Lancer l'API
docker run -p 8000:8000 credit_scoring_api:latest
```

## ğŸ“Š Monitoring

- Le dashboard Evidently AI est gÃ©nÃ©rÃ© Ã  chaque batch ou via triggers de recalibrage automatique :
``dashboards/credit_model_monitoring.html``
- Permet de suivre la dÃ©rive de donnÃ©es et la performance du modÃ¨le.

## âš¡ CI/CD

- GitHub Actions ou Jenkins dÃ©clenchent :
    - Build Docker
    - Tests unitaires
    - DÃ©ploiement de lâ€™API sur serveur ou cluster

## ğŸ”„ Recalibrage pÃ©riodique

- BasÃ© sur seuils de dÃ©rive ou mÃ©triques (AUC / F1) via Evidently / MLflow.
- Permet un rÃ©entraÃ®nement automatique pour maintenir la performance en production.

## ğŸ“Œ Remarques
- Compatible avec Spark, MLflow, FastAPI, ONNX, Docker, Evidently AI.
- PrÃªt pour un dÃ©ploiement local ou cloud (AWS, GCP, Azure).
